package alexp.macrobase.pipeline.benchmark;

import alexp.macrobase.evaluation.GridSearch;
import alexp.macrobase.evaluation.memory.BasicMemoryProfiler;
import alexp.macrobase.explanation.Explanation;
import alexp.macrobase.outlier.Trainable;
import alexp.macrobase.outlier.Updatable;
import alexp.macrobase.outlier.loda.loda;
import alexp.macrobase.outlier.xstream.Xstream;
import alexp.macrobase.pipeline.Pipeline;
import alexp.macrobase.pipeline.Pipelines;
import alexp.macrobase.pipeline.benchmark.config.*;
import alexp.macrobase.pipeline.benchmark.result.ExecutionResult;
import alexp.macrobase.pipeline.benchmark.result.ResultFileWriter;
import alexp.macrobase.pipeline.benchmark.result.ResultHolder;
import alexp.macrobase.pipeline.benchmark.result.ResultWriter;
import alexp.macrobase.pipeline.config.StringObjectMap;
import alexp.macrobase.streaming.StreamGenerator;
import alexp.macrobase.streaming.Windows.WindowManager;
import alexp.macrobase.utils.BenchmarkUtils;
import com.google.common.base.Strings;
import com.google.common.collect.Iterables;
import edu.stanford.futuredata.macrobase.analysis.classify.Classifier;
import edu.stanford.futuredata.macrobase.datamodel.DataFrame;
import edu.stanford.futuredata.macrobase.datamodel.Schema;
import org.apache.commons.io.FilenameUtils;

import java.util.*;
import java.util.function.Function;
import java.util.stream.Collectors;

import static alexp.macrobase.utils.BenchmarkUtils.aucCurve;
import alexp.macrobase.utils.DataFrameUtils;
import java.io.File;
import java.io.FileWriter;
import java.io.IOException;
import org.apache.commons.lang3.ArrayUtils;

public class BenchmarkPipeline extends Pipeline {

    private final ExecutionType executionType;

    private final ExecutionConfig conf;
    private final String rootDataDir;
    private ResultWriter resultWriter;
    private final String timeColumn = "__autogenerated_time";
    private DataFrame dataFrame;
    private int[] labels;

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf) {
        this(executionType, conf, null, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir) {
        this(executionType, conf, rootDataDir, null);
    }

    public BenchmarkPipeline(ExecutionType executionType, ExecutionConfig conf, String rootDataDir, ResultWriter resultWriter) {
        this.executionType = executionType;
        this.conf = conf;
        this.rootDataDir = rootDataDir;
        this.resultWriter = resultWriter;
    }

    public void run() throws Exception {
        if (resultWriter == null) {
            setupResultWriter();
        }

        switch (executionType) {
            case BATCH_CLASSIFICATION:
                classificationMode();
                break;
            case STREAMING_CLASSIFICATION:
                streamingMode();
                break;
            case EXPLANATION:
                explanationMode();
                break;
        }
    }

    private void classificationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();

        printInfo(String.format("Running %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);
        StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf);

        if (!algorithmParameters.equals(classifierConf.getParameters())) {
            out.println(algorithmParameters);
        }
        Classifier classifier = Pipelines.getClassifier(
                classifierConf.getAlgorithmId(),
                algorithmParameters,
                conf.getDatasetConfig().getMetricColumns(),
                conf.getDatasetConfig().getDatasetId());
        ResultHolder resultHolder = runClassifier(classifier);

        printInfo(String.format("Training time: %d ms (%.2f sec), Classification time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
                resultHolder.getTrainingTime(), resultHolder.getTrainingTime() / 1000.0,
                resultHolder.getClassificationTime(), resultHolder.getClassificationTime() / 1000.0,
                resultHolder.getMaxMemoryUsage() / 1024 / 1024,
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).rocArea()),
                labels == null ? "n/a" : String.format("%.2f", aucCurve(resultHolder.getResultsDf().getDoubleColumnByName(classifier.getOutputColumnName()), labels).prArea())));

        resultWriter.write(resultHolder.getResultsDf(),
                new ExecutionResult(resultHolder.getTrainingTime(), resultHolder.getClassificationTime(),
                        0,
                        resultHolder.getMaxMemoryUsage(), conf, algorithmParameters)
                        .setClassifierId(classifierConf.getAlgorithmId())
        );
    }

    private void streamingMode() throws Exception {
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //
        List<List<Long>> streamTrainTimeReps = new ArrayList<>();
        List<List<Long>> streamPredictTimeReps = new ArrayList<>();
        List<List<Long>> streamUpdateTimeReps = new ArrayList<>();
        List<Long> streamMemoryPeakReps = new ArrayList<>();
        List<DataFrame> streamDFReps = new ArrayList<>();
        List<DataFrame> streamMIReps = new ArrayList<>();
        List<StringObjectMap> streamMPReps = new ArrayList<>();
        List<Double> performanceReps = new ArrayList<>();
        List<Double> AUCperformanceReps = new ArrayList<>();
        List<double[]> streamScores = new ArrayList<>();
        Classifier clf = null;
        // - - - - - - - - - - - - - - - - - - - - - - - - - - //

        int streaming_reps = 30;
        // Repeat the streaming n times (that is because of non-deterministic algorithms)
        for (int rep = 0; rep < streaming_reps; rep++) {
            System.out.println("Streaming mode: ON. REP = " + rep);
            System.out.println("Working Directory = " + System.getProperty("user.dir"));
            // - - - - - - - - - - - - - - - - - - - - - - - - - - //
            DataFrame streamDF;
            DataFrame streamMI;
            final List<Long> streamTrainTime = new ArrayList<>();
            final List<Long> streamPredictTime = new ArrayList<>();
            final List<Long> streamUpdateTime = new ArrayList<>();
            BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
            StreamGenerator.init();
            String rawDataPoint = "";
            // - - - - - - - - - - - - - - - - - - - - - - - - - - //
            AlgorithmConfig classifierConf = conf.getClassifierConfig();
            printInfo(String.format("Running %s %s on %s", classifierConf.getAlgorithmId(), classifierConf.getParameters(), conf.getDatasetConfig().getUri().getOriginalString()));
            if (resultWriter == null) { // Print the classifier information
                setupResultWriter();
            }
            StringObjectMap algorithmParameters = getAlgorithmParameters(classifierConf); // Validate the algorithm parameters
            if (!algorithmParameters.equals(classifierConf.getParameters())) {
                out.println(algorithmParameters);
            }
            WindowManager wm = new WindowManager(classifierConf, conf.getDatasetConfig()); // Initialize window manager
            String windowType = wm.getWindowMethod();
            Classifier streamingClassifier = Pipelines.getClassifier( // Build the Streaming Classifier Model
                    classifierConf.getAlgorithmId(),
                    algorithmParameters,
                    conf.getDatasetConfig().getMetricColumns(),
                    conf.getDatasetConfig().getDatasetId()
            );
            int points;
            if(conf.getClassifierConfig().getAlgorithmId().equals("mcod") || conf.getClassifierConfig().getAlgorithmId().equals("loda")){
                points = 10000000;
            }
            else{
                points = (int) (Integer.parseInt(conf.getClassifierConfig().getParameters().getValues().get("trainSize").toString()) * 2.5);
            }
            clf = streamingClassifier;
            if (windowType.equals("none")) { // Make sure that the current classifier is streaming classifier.
                return;
            }
            int windows = 0;
            FileWriter fileWriter = new FileWriter("TEST.csv", false);
            while (true) { // Iteratively Repeat (Streaming Simulation)
                if (!wm.windowIsConstructed()) {
                    rawDataPoint = StreamGenerator.fetch(conf.getDatasetConfig().getUri().getPath()); // Read a raw data point from the Stream Generator
                    points -= 1;
                    wm.manage(rawDataPoint); // Obtain the window when the window method invariants are satisfied

                    if (wm.getWindowSize() <= 0) {
                        break; // Stop streaming simulation when the real size of the window is zero
                    }

                } else {

                    dataFrame = wm.getWindowDF(); // Build the window DataFrame

                    createAutoGeneratedColumns(dataFrame, timeColumn); // Add a time column to the DataFrame
                    windows++;
                    if(classifierConf.getAlgorithmId().equals("loda")){
                        try
                        {
                            String filename= "TEST.csv";
                            List<double[]> data = (DataFrameUtils.toRowArray(dataFrame, conf.getDatasetConfig().getMetricColumns()));
                            FileWriter fw = new FileWriter(filename,true); //the true will append the new data
                            int j;
                            for(int i=0; i < dataFrame.getNumRows(); i++){
                                for(j=dataFrame.getRow(i).getVals().size()-2; j > 1; j--){
                                    fw.write(dataFrame.getRow(i).getVals().get(j).toString() + ",");
                                }
                                fw.write(dataFrame.getRow(i).getVals().get(j).toString());
                                fw.write("\n");
                            }
                            fw.close();
                        }
                        catch(IOException ioe)
                        {
                            System.err.println("IOException: " + ioe.getMessage());
                        }
                    }

                    streamTrainTime.add(streamingClassifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
                        ((Trainable) streamingClassifier).train(dataFrame);
                    }) : 0);

                    streamPredictTime.add(BenchmarkUtils.measureTime(() -> {
                        streamingClassifier.process(dataFrame);
                    }));

                    streamUpdateTime.add(streamingClassifier instanceof Updatable ? BenchmarkUtils.measureTime(() -> {
                        ((Updatable) streamingClassifier).update(dataFrame);
                    }) : 0);

                    wm.clearWindowData();  // Clear the window data (in order to continue to the next window construction)

                    if (wm.isEndStream()) {
                        break; // Stop streaming simulation when the generator is empty
                    }

                }
            } // End of streaming simulation
            /*
            if(classifierConf.getAlgorithmId().equals("xstream")){
                ((Xstream) streamingClassifier).setNumWindows(windows);
                ((Xstream) streamingClassifier).run_xstream("TEST.svm");
                ((Xstream) streamingClassifier).closeWriter();
            }*/

            if(classifierConf.getAlgorithmId().equals("loda")){
                ((loda) streamingClassifier).run_loda("TEST.csv");
            }
            long streamMemoryPeak = memoryProfiler.getPeakUsage();
            streamDF = streamingClassifier.getResults();
            streamMI = streamingClassifier.getModelInfo();
            if (streamDF != null) {
                double[] scores = streamDF.getDoubleColumnByName(streamingClassifier.getOutputColumnName());
                double performance = aucCurve(scores, getLabels(streamDF)).prArea();
                System.out.println(performance);
                double auc = aucCurve(scores, getLabels(streamDF)).rocArea();
                System.out.println(auc);
                int[] labels = getLabels(streamDF);
                double[] scores_tmp;
                int[] labels_tmp;
                double[] auc_scores = {};
                double[] ap_scores = {};
                int window_size = 128;
                
                for(int i = 0; i < scores.length; i++){
                    if(i+window_size-1 >= scores.length){
                        scores_tmp = Arrays.copyOfRange(scores, i, scores.length-1);
                        labels_tmp = Arrays.copyOfRange(labels, i, scores.length-1);
                        auc_scores = ArrayUtils.add(auc_scores, aucCurve(scores_tmp, labels_tmp).rocArea());
                        ap_scores = ArrayUtils.add(ap_scores, aucCurve(scores_tmp, labels_tmp).prArea());
                        break;
                    }
                    scores_tmp = Arrays.copyOfRange(scores, i, i+window_size-1);
                    labels_tmp = Arrays.copyOfRange(labels, i, i+window_size-1);
                    i = i + window_size;
                    auc_scores = ArrayUtils.add(auc_scores, aucCurve(scores_tmp, labels_tmp).rocArea());
                    ap_scores = ArrayUtils.add(ap_scores, aucCurve(scores_tmp, labels_tmp).prArea());

                }
                double sum = 0;
                double sum_ap = 0;
                for(int i = 0; i < auc_scores.length; i++){
                    if(Double.isNaN(auc_scores[i])){
                        continue;
                    }
                    sum = sum + auc_scores[i];
                    sum_ap = sum_ap + ap_scores[i];
                }
                auc = sum / auc_scores.length;
                performance = sum_ap/ap_scores.length;

                streamScores.add(scores);
                streamTrainTimeReps.add(streamTrainTime);
                streamPredictTimeReps.add(streamPredictTime);
                streamUpdateTimeReps.add(streamUpdateTime);
                streamMemoryPeakReps.add(streamMemoryPeak);
                streamDFReps.add(streamDF);
                streamMIReps.add(streamMI);
                streamMPReps.add(algorithmParameters);
                AUCperformanceReps.add(auc);
                performanceReps.add(performance);
            } else {
                break;
            }
            // Continue to the next rep..
        }

        // Repetitions has been completed.
        if (!streamDFReps.isEmpty()) {
            // the final selected model index
            int finalModelIndex = 0;

            if(Objects.equals(conf.getClassifierConfig().getAlgorithmId(), "mcod")){
                Double maxVal = Collections.max(performanceReps);
                finalModelIndex = performanceReps.indexOf(maxVal);

            }else {

                // Calculate the average scores
                double[] scoresAVG = new double[streamScores.get(0).length];
                for (int col = 0; col < streamScores.get(0).length; col++) {
                    double colSum = 0;
                    for (double[] streamScore : streamScores) {
                        colSum += streamScore[col];
                    }
                    scoresAVG[col] = colSum / streaming_reps;
                }
                // Calculate the average performance
                double performanceAVG = aucCurve(scoresAVG, getLabels(streamDFReps.get(0))).prArea();
                // Find the index of the model that its performance is closer to the average performance
                double[] performanceDistances = new double[performanceReps.size()];
                for (int idx = 0; idx < performanceReps.size(); idx++) {
                    performanceDistances[idx] = Math.abs(performanceAVG - performanceReps.get(idx));
                }
                double minValue = performanceDistances[finalModelIndex];
                for (int idx = 0; idx < performanceDistances.length; idx++) {
                    if (minValue > performanceDistances[idx]) {
                        minValue = performanceDistances[idx];
                        finalModelIndex = idx;
                    }
                }

            }
            // Find the information of the average model
            double modelPerformance = performanceReps.get(finalModelIndex);
            double AUCmodelPerformance = AUCperformanceReps.get(finalModelIndex);
            long modelTTime = avg(streamTrainTimeReps.get(finalModelIndex));
            long modelPTime = avg(streamPredictTimeReps.get(finalModelIndex));
            long modelUTime = avg(streamUpdateTimeReps.get(finalModelIndex));
            long modelMPeak = streamMemoryPeakReps.get(finalModelIndex);
            DataFrame model = streamDFReps.get(finalModelIndex);
            DataFrame modelInfo = streamMIReps.get(finalModelIndex);
            StringObjectMap modelParams = streamMPReps.get(finalModelIndex);

            String filename= "Results.csv";
            FileWriter fw = new FileWriter(filename,true);
            fw.write(conf.getClassifierConfig().getAlgorithmId() + "," + conf.getDatasetConfig().getDatasetId() + "," + conf.getClassifierConfig().getParameters().toString() + "," + modelPerformance + String.format("," +
                            "Training time: %f sec, " +
                            "Classification time: %f sec, " +
                            "Update time: %f sec, " +
                            "Max memory usage: %d MB, " +
                            "ROC AP: %.4f" + ",ROC AUC: %.4f",
                    ((double) modelTTime / 1000.0),
                    ((double) modelPTime / 1000.0),
                    ((double) modelUTime / 1000.0),
                    (modelMPeak / 1024 / 1024),
                    (modelPerformance),
                    (AUCmodelPerformance)
            )+ "\n");
            fw.close();
            // Write the results of the average model
            resultWriter.write(model, new ExecutionResult(modelTTime, modelPTime, modelUTime, modelMPeak, conf, modelParams));
            // Write the model info
            if (modelInfo != null) {
                clf.setModelInfo(modelInfo);
            }
            // Print results
            printInfo(String.format("" +
                            "Training time: %f sec, " +
                            "Classification time: %f sec, " +
                            "Update time: %f sec, " +
                            "Max memory usage: %d MB, " +
                            "ROC AP: %.4f" + " ROC AUC: %.4f",
                    ((double) modelTTime / 1000.0),
                    ((double) modelPTime / 1000.0),
                    ((double) modelUTime / 1000.0),
                    (modelMPeak / 1024 / 1024),
                    (modelPerformance),
                    (AUCmodelPerformance)
            ));
        } else {
            System.out.println("[Alert] There are no data points processed by the current algorithm");
        }
        // End of streaming mode
    }

    private static long avg(List<Long> list) {
        long sum = 0;
        for (long i : list) {
            sum += i;
        }
        return sum / list.size();
    }

    private void explanationMode() throws Exception {
        AlgorithmConfig classifierConf = conf.getClassifierConfig();
        AlgorithmConfig explainerConf = conf.getExplainerConfig();

        printInfo(String.format("Running Classifier %s %s + Explainer %s %s on %s",
                classifierConf.getAlgorithmId(), classifierConf.getParameters(),
                explainerConf.getAlgorithmId(), explainerConf.getParameters(),
                conf.getDatasetConfig().getUri().getOriginalString()));

        dataFrame = loadData();
        labels = getLabels(dataFrame);

        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();

        Explanation explainer = Pipelines.getExplainer(explainerConf, classifierConf, conf.getDatasetConfig(), conf.getSettingsConfig().getExplanationSettings());
        final long explanationTime = BenchmarkUtils.measureTime(() -> {
            explainer.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        System.out.println("\nTime " + explanationTime / 1000.0 + " sec");
//                printInfo(String.format("Explanation time: %d ms (%.2f sec), Max memory usage: %d MB, ROC AUC: %s, PR AUC: %s",
//                        explanationTime, explanationTime / 1000.0,
//                        maxMemoryUsage / 1024 / 1024,
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).rocArea()),
//                        labels == null ? "n/a" : String.format("%.2f", aucCurve(results.getDoubleColumnByName(explainer.getOutputColumnName()), labels).prArea())));
        resultWriter.write(explainer.getResults(), new ExecutionResult(0, explanationTime, 0, maxMemoryUsage,
                conf, explainerConf.getParameters())
                .setClassifierId(classifierConf.getAlgorithmId())
                .setExplainerId(explainerConf.getAlgorithmId()));
    }

    private ResultHolder runClassifier(Classifier classifier) throws Exception {
        BasicMemoryProfiler memoryProfiler = new BasicMemoryProfiler();
        final long trainingTime = classifier instanceof Trainable ? BenchmarkUtils.measureTime(() -> {
            ((Trainable) classifier).train(dataFrame);
        }) : 0;
        final long classificationTime = BenchmarkUtils.measureTime(() -> {
            classifier.process(dataFrame);
        });
        long maxMemoryUsage = memoryProfiler.getPeakUsage();
        DataFrame resultsDf = classifier.getResults();
        return new ResultHolder(resultsDf, trainingTime, classificationTime, maxMemoryUsage);
    }

    private StringObjectMap getAlgorithmParameters(AlgorithmConfig algorithmConfig) throws Exception {
        StringObjectMap baseParams = algorithmConfig.getParameters();
        GridSearchConfig gridSearchConfig = algorithmConfig.getGridSearchConfig();
        if (gridSearchConfig == null) {
            return baseParams;
        }
        out.println(String.format("Running Grid Search, using %s measure", gridSearchConfig.getMeasure().toUpperCase()));
        GridSearch gs = new GridSearch();
        gs.addParams(gridSearchConfig.getParameters());
        gs.setOutputStream(out);
        gs.run(params -> {
            StringObjectMap currentParams = baseParams.merge(params);
            Classifier classifier = Pipelines.getClassifier(
                    algorithmConfig.getAlgorithmId(),
                    currentParams,
                    conf.getDatasetConfig().getMetricColumns(),
                    conf.getDatasetConfig().getDatasetId());
            classifier.process(dataFrame);
            DataFrame classifierResultDf = classifier.getResults();
            double[] classifierResult = classifierResultDf.getDoubleColumnByName(classifier.getOutputColumnName());
            switch (gridSearchConfig.getMeasure()) {
                case "roc":
                    return aucCurve(classifierResult, labels).rocArea();
                case "pr":
                    return aucCurve(classifierResult, labels).prArea();
                default:
                    throw new RuntimeException("Unknown search measure " + gridSearchConfig.getMeasure());
            }
        });
        SortedMap<Double, Map<String, Object>> gsResults = gs.getResults();
        return baseParams.merge(new StringObjectMap(Iterables.getLast(gsResults.values())));
    }

    private void setupResultWriter() {
        resultWriter = new ResultFileWriter(executionType)
                .setOutputDir(getOutputDir())
                .setBaseFileName(FilenameUtils.removeExtension(conf.getDatasetConfig().getDatasetId())); // doesn't matter, currently always initialized in constructor
    }

    private DataFrame loadData() throws Exception {
        Map<String, Schema.ColType> colTypes = getColTypes();

        List<String> requiredColumns = new ArrayList<>(colTypes.keySet());

        DataFrame dataFrame = Pipelines.loadDataFrame(conf.getDatasetConfig().getUri().addRootPath(rootDataDir), colTypes, requiredColumns, conf.getDatasetConfig().toMap());

        createAutoGeneratedColumns(dataFrame, timeColumn); // needed for MCOD

        return dataFrame;
    }

    private int[] getLabels(DataFrame dataFrame) {
        String labelColumn = conf.getDatasetConfig().getLabelColumn();
        if (Strings.isNullOrEmpty(labelColumn)) {
            return null;
        }

        return Arrays.stream(dataFrame.getDoubleColumnByName(labelColumn))
                .mapToInt(d -> (int) d)
                .toArray();
    }

    private Map<String, Schema.ColType> getColTypes() {
        Map<String, Schema.ColType> colTypes = Arrays.stream(conf.getDatasetConfig().getMetricColumns())
                .collect(Collectors.toMap(Function.identity(), c -> Schema.ColType.DOUBLE));

        if (!Strings.isNullOrEmpty(conf.getDatasetConfig().getLabelColumn())) {
            colTypes.put(conf.getDatasetConfig().getLabelColumn(), Schema.ColType.DOUBLE);
        }

        return colTypes;
    }

}
